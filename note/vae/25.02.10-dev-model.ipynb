{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd ~/loutrebleu/menta/stroker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.expanduser(\"~/loutrebleu/menta/stroker\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import xtools as xt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as toptim\n",
    "import torch.utils as tutils\n",
    "import torchvision.datasets as tdatasets\n",
    "import torchvision.transforms as ttransforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, List\n",
    "from collections import namedtuple\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stroker.tools.dataloader import new_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'path': '../images/marker/dev1',\n",
       "  'transforms': {'crop': [128, 128], 'resize': [64, 64]},\n",
       "  'batch_size': 16,\n",
       "  'shuffle': True}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = \"config.yaml\"\n",
    "cf = xt.Config(config_file)\n",
    "cf._cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = new_dataloader(cf.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEVEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_shape: Tuple[int] | List[int] | np.ndarray,\n",
    "            num_layer: int,\n",
    "            base_channel: int,\n",
    "            latent_size: int = 50,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.image_shape = np.asarray(image_shape)\n",
    "        self.num_layer = num_layer\n",
    "        ch = np.power(2, np.arange(num_layer)) * base_channel\n",
    "        ch = np.insert(ch, 0, image_shape[0])\n",
    "        self.ch = ch\n",
    "\n",
    "        self.shape_after_conv = shape_after_conv = (self.image_shape[1:] / (2 ** num_layer) / 2)\n",
    "        self.size_after_conv = size_after_conv = int(shape_after_conv[0] * shape_after_conv[1] * ch[-1])\n",
    "\n",
    "        self.convs = nn.Sequential(*[\n",
    "            self._make_layer(ich, och)\n",
    "            for ich, och in zip(ch[:-1], ch[1:])\n",
    "        ])\n",
    "\n",
    "        self.layer_flt = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(size_after_conv, 100),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_ave = nn.Linear(100, latent_size)\n",
    "        self.layer_dev = nn.Linear(100, latent_size)\n",
    "    \n",
    "    def _make_layer(self, in_ch, out_ch=None):\n",
    "        out_ch = 2 * in_ch if out_ch is None else out_ch\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.layer_flt(x)\n",
    "\n",
    "        mean = self.layer_ave(x)\n",
    "        log_dev = self.layer_dev(x)\n",
    "\n",
    "        eps = torch.rand_like(mean)\n",
    "        feat = mean + torch.exp(log_dev / 2) * eps\n",
    "        return feat, mean, log_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEVDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: DEVEncoder) -> None:\n",
    "        super().__init__()\n",
    "        ch = encoder.ch[::-1]\n",
    "        ch[-1] = ch[-2]\n",
    "        self.ch = ch\n",
    "\n",
    "        self.shape_resize_to = np.asarray([ch[0], *encoder.shape_after_conv]).astype(int)\n",
    "        print(ch)\n",
    "        print(\"shape_resize_to:\", self.shape_resize_to)\n",
    "\n",
    "        self.layer_latent = nn.Sequential(\n",
    "            nn.Linear(encoder.latent_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, encoder.size_after_conv),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convs = nn.Sequential(*[\n",
    "            self._make_layer(ich, och)\n",
    "            for ich, och in zip(ch[:-1], ch[1:])\n",
    "        ])\n",
    "        self.layer_recons = nn.Sequential(\n",
    "            nn.Conv2d(ch[-1], encoder.image_shape[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, in_ch, out_ch=None):\n",
    "        if out_ch is None:\n",
    "            out_ch = in_ch // 2\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.layer_latent(x)\n",
    "        feat = feat.view(x.shape[0], *self.shape_resize_to)\n",
    "        \n",
    "        xcv = self.convs(feat)\n",
    "        recons = self.layer_recons(xcv)\n",
    "\n",
    "        return recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stroker.model.encoder import Encoder\n",
    "import stroker.model.encoder as libenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = libenc.Encoder([1, 64, 64], 2, 8, latent_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in loader:\n",
    "    latent, mean, log_dev = encoder(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stroker.model.decoder import Decoder\n",
    "import stroker.model.decoder as libdec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = libdec.Decoder(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 64, 64]) torch.Size([16, 1, 64, 64])\n",
      "torch.Size([16, 1, 64, 64]) torch.Size([16, 1, 64, 64])\n",
      "torch.Size([16, 1, 64, 64]) torch.Size([16, 1, 64, 64])\n",
      "torch.Size([4, 1, 64, 64]) torch.Size([4, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in loader:\n",
    "    latent, mean, log_dev = encoder(images)\n",
    "    recons = decoder(latent)\n",
    "    print(images.shape, recons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEData:\n",
    "\n",
    "    def __init__(self, recons, latent, mean, log_dev):\n",
    "        self.recons = recons\n",
    "        self.latent = latent\n",
    "        self.mean = mean\n",
    "        self.log_dev = log_dev\n",
    "\n",
    "        self._loss = None\n",
    "        self._rc_loss = None\n",
    "        self._kl_loss = None\n",
    "    \n",
    "    def loss(self, target):\n",
    "        rc_loss = nn.functional.binary_cross_entropy(self.recons, target, reduction=\"sum\")\n",
    "        kl_loss = -1/2 * torch.sum(1 + self.log_dev - self.mean ** 2 - self.log_dev.exp())\n",
    "        self._loss = loss = rc_loss + kl_loss\n",
    "        self._kl_loss = kl_loss\n",
    "        self._rc_loss = rc_loss\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def kl_loss(self):\n",
    "        return self._kl_loss.item()\n",
    "    \n",
    "    @property\n",
    "    def rc_loss(self):\n",
    "        return self._rc_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_shape: Tuple[int] | List[int] | np.ndarray,\n",
    "            num_layer: int,\n",
    "            base_channel: int,\n",
    "            latent_size: int = 50,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = libenc.Encoder(image_shape, num_layer, base_channel, latent_size)\n",
    "        self.decoder = libdec.Decoder(self.encoder)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent, mean, log_dev = self.encoder(x)\n",
    "        recons = self.decoder(latent)\n",
    "        data = VAEData(recons, latent, mean, log_dev)\n",
    "        return data\n",
    "    \n",
    "    def encode(self, images):\n",
    "        if not torch.is_tensor(images):\n",
    "            images = torch.from_numpy(images).float()\n",
    "        if images.ndim == 2:\n",
    "            images = torch.unsqueeze(images, 0)\n",
    "        if images.ndim == 3:\n",
    "            images = torch.unsqueeze(images, 0)\n",
    "        latent, _, _ = self.encoder(images)\n",
    "        return latent.detach().numpy()\n",
    "    \n",
    "    def decode(self, latents):\n",
    "        if not torch.is_tensor(latents):\n",
    "            latents = torch.from_numpy(latents)\n",
    "        if latents.ndim == 1:\n",
    "            latents = latents.unsqueeze(latents, 0)\n",
    "        recons = self.decoder(latents)\n",
    "        return recons.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE([1, 64, 64], 2, 4, 10)\n",
    "len(list(vae.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent: (1, 10) float32\n",
      "recons: (1, 1, 64, 64) float32\n"
     ]
    }
   ],
   "source": [
    "for images, labels in loader:\n",
    "    image = images[0, ...]\n",
    "    latent = vae.encode(image)\n",
    "    print(\"latent:\", latent.shape, latent.dtype)\n",
    "    recons = vae.decode(latent)\n",
    "    print(\"recons:\", recons.shape, recons.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
